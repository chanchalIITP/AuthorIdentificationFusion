# -*- coding: utf-8 -*-
"""MultiTask_50Tweets_20Mail.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gB_58NS_1F557Z632P3nIiqmC7g9YZun
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets,transforms
import numpy as np
import matplotlib.pyplot as plt
import os
import pandas as pd
from keras.layers import merge
from functools import partial
import tensorflow as tf
from tensorflow.keras.layers import LSTM, GRU

#import os
#os.environ["CUDA_VISIBLE_DEVICES"] = "-1"


import tensorflow as tf
from tensorflow.python.ops import nn
from tensorflow.python.keras import activations, regularizers, initializers, constraints, engine
from tensorflow.python.keras.utils import conv_utils
from tensorflow.python.keras.layers import Layer, deserialize, Conv1D
from tensorflow.python.keras import backend as K
from tensorflow.python.ops import array_ops


__all__ = ['KernelConv2D']


tester_data = pd.read_csv("1000_tweets_per_user_train.csv", encoding='utf-8')
#tester_data = pd.read_csv('1000_tweets_per_user_train.csv', encoding='utf-8')
print(tester_data.head(10))
tester_data = tester_data.sample(frac=0.1)
tester_data.info()





#tester_data.rename(columns = {'Text':'text', 'label':'authors'}, inplace = True) 
tester_data.rename(columns = {'0':'text', '1':'authors'}, inplace = True)

'''
remove_n = 40000

drop_indices = np.random.choice(tester_data.index, remove_n, replace=False)
tester_data = tester_data.drop(drop_indices)
'''
print(tester_data['authors'].value_counts())

print(tester_data.info())
print(email_data.info())

# tester_data['authors'].value_counts()

from tensorflow.python.keras.models import Sequential,Model
from tensorflow.python.keras.layers import Flatten, Dense,AveragePooling1D,GRU,Convolution1D, MaxPooling1D, AveragePooling1D,Embedding,Input, Dense, Dropout, Flatten

class GaussianKernel(Layer):

    def __init__(self, gamma, **kwargs):
        super(GaussianKernel, self).__init__(**kwargs)
        self.gamma = gamma

    def call(self, inputs, **kwargs):
        x, kernel = K.expand_dims(inputs[0], axis=-1), inputs[1]
        return K.exp(-self.gamma * K.sum(K.square(x - kernel), axis=-2))

    def get_config(self):
        config = {'gamma': self.gamma}
        base_config = super(GaussianKernel, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))
class PolynomialKernel(Layer):

    def __init__(self, p,
                 c=0.0,
                 trainable_c=False,
                 initializer='zeros',
                 regularizer=None,
                 constraint=None,
                 **kwargs):
        super(PolynomialKernel, self).__init__(**kwargs)
        self.p = p
        self.c = c
        self.oc = c
        self.trainable_c = trainable_c
        self.initializer = initializers.get(initializer)
        self.regularizer = regularizers.get(regularizer)
        self.constraint = constraints.get(constraint)

    def build(self, input_shape):
        if self.trainable_c:
            self.c = self.add_weight(
                shape=(),
                initializer=self.initializer,
                regularizer=self.regularizer,
                constraint=self.constraint,
                name='{}_c'.format(self.name),
            )
        super(PolynomialKernel, self).build(input_shape)

    def call(self, inputs, **kwargs):
        return (K.dot(inputs[0], inputs[1]) + self.c) ** self.p

    def get_config(self):
        config = {
            'p': self.p,
            'c': self.oc,
            'trainable_c': self.trainable_c,
            'initializer': initializers.serialize(self.initializer),
            'regularizer': regularizers.serialize(self.regularizer),
            'constraint': initializers.serialize(self.constraint),
        }
        base_config = super(PolynomialKernel, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

import pandas as pd
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(tester_data['text'], tester_data['authors'], test_size=0.33, random_state=0)

print(X_train.shape,y_train.shape,X_test.shape,y_test.shape)
#print(dataset_30.shape)

'''
k=0
X_train=[]
y_train=[]
for i in range(0,30):
  for j in range(k,k+900):
    X_train.append(dataset.iloc[j,0])
    y_train.append(dataset.iloc[j,1])
  k+=1000

k=900
X_test=[]
X_test.append(dataset.iloc[j,0])
    y_test.append(dataset.iloc[j,1])
  k+=1000
'''

X_train=pd.DataFrame(X_train)
X_test=pd.DataFrame(X_test)
y_train=pd.DataFrame(y_train)
y_test=pd.DataFrame(y_test)


print('Tweet: After Changing to Dataframe ', X_train.shape,y_train.shape,X_test.shape,y_test.shape)
print('Mail: After Changing to Dataframe ', X_train_ml.shape,y_train_ml.shape,X_test_ml.shape,y_test_ml.shape)

X_train=X_train.iloc[:,:].values
X_test=X_test.iloc[:,:].values
y_train=y_train.iloc[:,:].values
y_test=y_test.iloc[:,:].values


print('Tweet: After Changing to Dataframe ', X_train.shape,y_train.shape,X_test.shape,y_test.shape)

train=np.concatenate((X_train,y_train),axis=1)
test=np.concatenate((X_test,y_test),axis=1)


print('Tweet: Train and Test size = ', train.shape, test.shape)

np.random.shuffle(train)
np.random.shuffle(test)

train=pd.DataFrame(train)
test=pd.DataFrame(test)

print(train.shape)
print(test.shape)

# train
print('Tweet: Train and Test size = ', train.shape, test.shape)
X = train[train.columns[0]]
y = train[train.columns[1:]]
print(X.shape)
print(y.shape)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
print(X_train.shape,y_train.shape,X_test.shape,y_test.shape)

'''
X_train=train.iloc[:,0]
X_test=test.iloc[:,0]
y_train=train.iloc[:,1]
y_test=test.iloc[:,1]
'''

from sklearn.preprocessing import LabelEncoder
from keras.utils import np_utils
encoder = LabelEncoder()
encoder.fit(y_train)
encoded_Y = encoder.transform(y_train)
# convert integers to dummy variables (i.e. one hot encoded)
y_train = np_utils.to_categorical(encoded_Y)

from sklearn.preprocessing import LabelEncoder
from keras.utils import np_utils
encoder = LabelEncoder()
encoder.fit(y_test)
encoded_Y = encoder.transform(y_test)
# convert integers to dummy variables (i.e. one hot encoded)
y_test = np_utils.to_categorical(encoded_Y)

def create_vocab_set():
    alphabet = (list(string.ascii_lowercase) + list(string.digits) + list(string.punctuation) + ['\n'] + [' '] )
    vocab_size = len(alphabet)
    check = set(alphabet)

    vocab = {}
    reverse_vocab = {}
    for ix, t in enumerate(alphabet):
        vocab[t] = ix
        reverse_vocab[ix] = t
    return vocab, reverse_vocab, vocab_size, check

import string

maxlen = 140
vocab, reverse_vocab, vocab_size, check =create_vocab_set()
print(vocab_size)

def encode_data(x, maxlen, vocab, vocab_size, check):

    input_data = np.zeros((len(x), maxlen, vocab_size))
    for dix, sent in enumerate(x):
        counter = 0
        sent_array = np.zeros((maxlen, vocab_size))
        chars = list(sent.lower())
        for c in chars:
            if counter >= maxlen:
                pass
            else:
                char_array = np.zeros(vocab_size, dtype=np.int)
                if c in check:
                    ix = vocab[c]
                    char_array[ix] = 1
                sent_array[counter, :] = char_array
                counter += 1
        input_data[dix, :, :] = sent_array

    return input_data

X_train = X_train.astype(str)
X_test = X_test.astype(str)
X_train = encode_data(X_train, maxlen, vocab, vocab_size, check)
X_test= encode_data(X_test, maxlen, vocab, vocab_size, check)


print('Tweet: Before ', X_train.shape,y_train.shape,X_test.shape,y_test.shape)

from sklearn.model_selection import train_test_split
X_train, X_eval, y_train, y_eval = train_test_split(X_train,y_train, test_size = 0.25, random_state = 0)

print('TweetA: fter ', X_train.shape,y_train.shape,X_test.shape,y_test.shape)

gru_len = 128
Routings = 5
Num_capsule = 10
Dim_capsule = 16
dropout_p = 0.3
rate_drop_dense = 0.3

def squash(x, axis=-1):
    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()
    scale = K.sqrt(s_squared_norm) / (0.5 + s_squared_norm)
    return scale * x


# define our own softmax function instead of K.softmax
# because K.softmax can not specify axis.
def softmax(x, axis=-1):
    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))
    return ex / K.sum(ex, axis=axis, keepdims=True)


# define the margin loss like hinge loss
def margin_loss(y_true, y_pred):
    lamb, margin = 0.5, 0.1
    return K.sum(y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * (
        1 - y_true) * K.square(K.relu(y_pred - margin)), axis=-1)

class Capsule(Layer):
    """A Capsule Implement with Pure Keras
    There are two vesions of Capsule.
    One is like dense layer (for the fixed-shape input),
    and the other is like timedistributed dense (for various length input).

    The input shape of Capsule must be (batch_size,
                                        input_num_capsule,
                                        input_dim_capsule
                                       )
    and the output shape is (batch_size,
                             num_capsule,
                             dim_capsule
                            )

    Capsule Implement is from https://github.com/bojone/Capsule/
    Capsule Paper: https://arxiv.org/abs/1710.09829
    """

    def __init__(self,
                 num_capsule,
                 dim_capsule,
                 routings=3,
                 share_weights=True,
                 activation='squash',
                 **kwargs):
        super(Capsule, self).__init__(**kwargs)
        self.num_capsule = num_capsule
        self.dim_capsule = dim_capsule
        self.routings = routings
        self.share_weights = share_weights
        if activation == 'squash':
            self.activation = squash
        else:
            self.activation = activations.get(activation)

    def build(self, input_shape):
        input_dim_capsule = input_shape[-1]
        if self.share_weights:
            self.kernel = self.add_weight(
                name='capsule_kernel',
                shape=(1, input_dim_capsule,
                       self.num_capsule * self.dim_capsule),
                initializer='glorot_uniform',
                trainable=True)
        else:
            input_num_capsule = input_shape[-2]
            self.kernel = self.add_weight(
                name='capsule_kernel',
                shape=(input_num_capsule, input_dim_capsule,
                       self.num_capsule * self.dim_capsule),
                initializer='glorot_uniform',
                trainable=True)

    def call(self, inputs):
        """Following the routing algorithm from Hinton's paper,
        but replace b = b + <u,v> with b = <u,v>.

        This change can improve the feature representation of Capsule.

        However, you can replace
            b = K.batch_dot(outputs, hat_inputs, [2, 3])
        with
            b += K.batch_dot(outputs, hat_inputs, [2, 3])
        to realize a standard routing.
        """

        if self.share_weights:
            hat_inputs = K.conv1d(inputs, self.kernel)
        else:
            hat_inputs = K.local_conv1d(inputs, self.kernel, [1], [1])

        batch_size = K.shape(inputs)[0]
        input_num_capsule = K.shape(inputs)[1]
        hat_inputs = K.reshape(hat_inputs,
                               (batch_size, input_num_capsule,
                                self.num_capsule, self.dim_capsule))
        hat_inputs = K.permute_dimensions(hat_inputs, (0, 2, 1, 3))

        b = K.zeros_like(hat_inputs[:, :, :, 0])
        for i in range(self.routings):
            c = softmax(b, 1)
            o = self.activation(K.batch_dot(c, hat_inputs, [2, 2]))
            if i < self.routings - 1:
                b = K.batch_dot(o, hat_inputs, [2, 3])
                if K.backend() == 'theano':
                    o = K.sum(o, axis=1)

        return o

    def compute_output_shape(self, input_shape):
        return (None, self.num_capsule, self.dim_capsule)

from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)
callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)



def model2(filter_kernels, dense_outputs, maxlen, vocab_size, nb_filter,cat_output):
    d = 300
    inputs = Input(shape=(maxlen, vocab_size), name='input', dtype='float32')
    conv1 = Convolution1D(filters=nb_filter, kernel_size=filter_kernels[0], activation='relu',input_shape=(maxlen, vocab_size))(inputs)
    conv2 = Convolution1D(filters=nb_filter, kernel_size=filter_kernels[1], activation='relu',input_shape=(maxlen, vocab_size))(inputs)
#    conv3 = Convolution1D(filters=nb_filter, kernel_size=filter_kernels[2], activation='relu',input_shape=(maxlen, vocab_size))(inputs)
    #conv4 = Convolution1D(filters=nb_filter, kernel_size=filter_kernels[3], activation='relu',input_shape=(maxlen, vocab_size))(inputs)


    conv1 = Convolution1D(filters=nb_filter, kernel_size=filter_kernels[0],
                           activation='relu')(conv1)
    conv2 = Convolution1D(filters=nb_filter, kernel_size=filter_kernels[1],
                           activation='relu')(conv2)

#    conv3 = Convolution1D(filters=nb_filter, kernel_size=filter_kernels[2],
#                           activation='relu')(conv3)

#    conv3 = Capsule(num_capsule=1 ,dim_capsule=72, routings=1,share_weights=True)(conv3)
	
    conv1 = LSTM(100)(conv1)

    conv2 = GRU(100)(conv2)

    conv1 = Flatten()(conv1)
    conv2 = Flatten()(conv2)
#    conv3 = Flatten()(conv3)

    conv_comb = tf.keras.layers.Concatenate(axis=1)([conv1, conv2])

    pred = Dense(cat_output, activation='softmax', name='output')(conv_comb)
    model = Model(inputs=inputs, outputs=pred)

    return model





print('Tweet: Last ', X_train.shape,y_train.shape,X_test.shape,y_test.shape)
nb_filter = 500
dense_outputs = 256
filter_kernels = [1,1,4,4,5]
cat_output = 50
cat_output_ml = 20
early_stopping = EarlyStopping(monitor='val_loss', patience=5)
model = model2(filter_kernels, dense_outputs,maxlen, vocab_size, nb_filter, cat_output)
print(model.summary())


model.compile(loss='categorical_crossentropy', optimizer="adam", metrics=['accuracy'])

model.fit(np.array(X_train), np.array(y_train),validation_data=(np.array(X_eval), np.array(y_eval)),batch_size=32,epochs=100, verbose=1,callbacks=[callback])
y_pred  = model.predict([np.array(X_test)])

y_pred=pd.DataFrame(y_pred)
y_pred=y_pred.eq(y_pred.where(y_pred != 0).max(1), axis=0).astype(int)
y_pred=y_pred.iloc[:,:].values

y_test=pd.DataFrame(y_test)
y_test=y_test.eq(y_test.where(y_test != 0).max(1), axis=0).astype(int)
y_test=y_test.iloc[:,:].values

result=[]
for i in range(0,len(y_test)):
  for j in range(0,len(y_test[0])):
    if(y_test[i][j]==1):
      result.append(j)

predicted=[]
for i in range(0,len(y_pred)):
  for j in range(0,len(y_pred[0])):
    if(y_pred[i][j]==1):
      predicted.append(j)

#print('Tweet: ', result)
#print('Tweet: ',predicted)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(result,predicted)


from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
print('Tweet Report')
print('Confusion Matrix :')
#print(cm)
print('Accuracy Score :',accuracy_score(result, predicted))
print('Report : ')
print(classification_report(result, predicted, digits=6))



file=open("heatmap_authorshipatribution2_new.html","w")
for i in range(0,40):
  type_here=[]
  type_here.append(X_train1[i])
  typr_here=pd.DataFrame(type_here)
  typr_here = encode_data2(type_here, maxlen, vocab, vocab_size, check)
  y_pred = model.predict(typr_here)
  Xtst=typr_here
  class_idx = np.argmax(y_pred[0]) #not needed in this case as only two classes
  class_output = model.output[:, class_idx]
  last_conv_layer = model.get_layer("conv1d_1")
  grads = K.gradients(class_output, last_conv_layer.output)[0]
  pooled_grads = K.mean(grads)
  iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])
  pooled_grads_value, conv_layer_output_value = iterate([Xtst])


  heatmap = np.mean(conv_layer_output_value, axis=-1)
  heatmap = np.maximum(heatmap,0)
  heatmap /= np.max(heatmap)#normalise values in the prediction
  norm_len = 36/last_conv_layer.output_shape[1]
  html = ""
  if y_pred[0][0]>0.5:
    pred = '90078731'
  else:
    pred = '51964081'
  html += "<span><h3>Based on the description, the model believes that text belongs to {} author ".format(pred)
  html += "<small><br>Confidence: {:.0f}%<br><br></small></h3></span>".format(abs(((y_pred[0][0]*100)-50)*2))
  for j,i in enumerate(type_here[0].split()):
    html += "<span style='background-color:rgba({},0,15,{})'>{} </span>".format(heatmap[math.floor(j/norm_len)]*255,heatmap[math.floor(j/norm_len)]-0.3,i)
  file.write(html)
file.close()
